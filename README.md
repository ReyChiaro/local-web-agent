# Local Web Agent

A lightweight, local Retrieval-Augmented Generation (RAG) system that extracts information from web pages and enables multi-turn question answering using a local language model.

> This project is partly generated by GPT-4o.

---

## 🔧 Features

- 🕸️ **Web content crawler**: Extracts text and resources from URLs
- 🔍 **Vector-based retrieval**: FAISS-based similarity search
- 🤖 **LLM integration**: Local inference with BitNet-1.58B (CPU supported)
- 🧩 **Multi-turn chat**: Maintains conversation context
- 🧱 **Modular design**: No LangChain or heavy frameworks

---

## 📁 Project Structure

```bash
.
├── README.md
├── app
│   ├── __init__.py
│   ├── agent.py
│   ├── conversation.py
│   ├── llm_wrapper.py
│   ├── prompt_builder.py
│   ├── query_refine.py
│   ├── retriever.py
│   └── run_chat.py             # The conversation app entrance 
├── config.py
├── crawler
│   ├── __init__.py
│   ├── fetch.py
│   └── parser.py
├── logs
├── models
│   ├── BitNet/                 # The BitNet repo, see https://github.com/microsoft/BitNet
│   ├── BitNet.py               # BitNet wrapper with python interfaces
│   └── __init__.py
├── scripts
│   ├── __init__.py
│   └── build_vectorstore.py    # Build vectorstore based on given URLs
├── start.py                    # The app start entrance
└── utils
    ├── __init__.py
    └── logger.py
```


---

## 🚀 Quick Start

### BitNet

For the balance of efficiency and effectiveness, this app uses the amazing work, [BitNet](https://github.com/microsoft/BitNet), as the backend model, which is based on [llama.cpp](https://github.com/ggml-org/llama.cpp) framework. [This repo](https://github.com/microsoft/BitNet) show the details of installation and deploy steps.

In order to integrate BitNet, the model should be moved to the folder `models/BitNet`, then `cd` to this folder and compile the BitNet project. You can find `llama-cli` excutable file in the `models/BitNet/build` folder and the model weights `*.gguf` in `models/BitNet/models`.

For a pratical suggestion, we first clone the BitNet repo and move it into `models/BitNet`:

```bash
git clone https://github.com/microsoft/BitNet.git
mv BitNet models/BitNet
```

Then, download the weights and compile the BitNet (take `BitNet-b1.58-2B-4T-gguf` for example):

```bash
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s
```

After that you will see `llama-cli` in `models/BitNet/build` and `ggml-model-i2_s.gguf` in `models/BitNet/models/BitNet-b1.58-2B-4T`, this means everything is ok for BitNet. These two arguments should be filled in `config.py`:

```python
# For example
MODEL_PATH = "models/BitNet/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf"
CLI_PATH = "models/BitNet/build/bin/llama-cli"
```

### Start the Agent

Just type the cmd in the project root dir:

```bash
python start.py <URL1> <URL2> .. OR path/to/urls.txt
```

then you will see the greetings from the agent.


## ⚙️ Configurations

We use `config.py` to manage all configurations of this project. Everything is well-commented and you can stylize the default settings to make your own agent.

```python
# URL parse
MAX_WORDS_PER_CHUNK = 100

# Vectorstore paths
DB_FOLDER = Path("vectorstore")
VECTOR_DB_PATH = DB_FOLDER / Path("index.faiss")
DOC_STORE_PATH = DB_FOLDER / Path("docs.pkl")

# Embedding model
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"

# Retrieval
LLM_NAME = "bitnet"
MODEL_PATH = "xxx"
CLI_PATH = "xxx"
TOP_K_DOCS = 5
MAX_TOKENS = 256
N_THREADS = 4
CTX_SIZE = 4096
TEMPERATURE = 0.8

# Loggings
LOG_FOLDER = Path("logs")
DB_LOG_PATH = LOG_FOLDER / Path("build_vectorstore.log")
CHAT_LOG_PATH = LOG_FOLDER / Path("chat.log")
```


## 🎲 Demo

1. Run `python start.py https://reychiaro.github.io/about` (which is my personal blog):

```bash
 ✅ (You can type `exit` or `quit` to end the conversation)
 🧑 : Given an abstract of the blog author
 😃 : Sure, here is an abstract based on the provided blog post:

    ---

    **Abstract of the Blog Author:**

    ---

    ReyChiaro, a research enthusiast with a strong interest in multimodal models, contrastive learning, and computer vision, shares insights and experiences from their journey as a research intern at Southeast University. The blog post, titled "about," delves into the author's educational background, current research projects, and publications. ReyChiaro highlights their involvement in various research initiatives, particularly focusing on multimodal models and their applications. They also discuss their collaborations, achievements, and future aspirations in the field of computer vision and multimodal learning.

    The author emphasizes the importance of academic exchange and open discussions in advancing research ideas and exploring new model architectures. They invite readers to reach out or follow them on GitHub for more insights and discussions.

    ---

    Feel free to ask if you need more information or have any other questions! 🌟🔬📚
```

(Though I am a student in Southeast University, not the research intern..., it can make sense, right?)

```bash
 🧑 : Tell me the education background of the author.
 😃 : The author is currently studying at Southeast University for a master degree. The specific field of study is not mentioned in the document provided. The document includes a link to the university's website, which provides more information about the program. Southeast University is a well-known institution in China, offering a range of undergraduate and graduate programs in various fields, including engineering, science, and technology. The document does not specify the exact field of study within the master's degree program, so it may be in a broad field such as computer science, engineering, or a related discipline. However, the author expresses a strong interest in multimodal models, contrastive learning, and computer vision, which suggests a focus on these areas.
```

(In this case, the agent repeat the answer over and over again.)

## ☑️ TODO

This project is still in develop, there are many steps we can take in the future:

- ⬜️ Beautify the ouputs
- ⬜️ Clean the input website contents without tags
- ⬜️ Integrate more powerful models into this project
- ⬜️ Multimodel support

